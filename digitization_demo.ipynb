{"cells":[{"cell_type":"markdown","source":["# Automating the Digitization of Drawn Figures on Maps"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["### Libraries/Dependencies used\r\n","\r\n","**OpenCV** for color extraction and image detection <br>\r\n","**Numpy** for numbers and array manipulation <br>\r\n","**glob, ntpath** for directory navigation <br>\r\n","**os** for environment variable use <br>\r\n","**azureml** for workspace connectivity and dataset creation and access"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["import numpy as np\r\n","import glob\r\n","import cv2\r\n","import ntpath as nt\r\n","import os"],"outputs":[],"execution_count":12,"metadata":{}},{"cell_type":"markdown","source":["### Subscription Information & Datastore Creation\r\n","\r\n","Necessary for workspace-storage connectivity and import of data\r\n","\r\n","Documentation: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data\r\n","\r\n"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Access to subscription information\r\n","sub_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"YOUR_SUBSCRIPTION_ID\")\r\n","rsc_group = os.getenv(\"RESOURCE_GROUP\", default=\"YOUR_RESOURCE_GROUP\")\r\n","ws_name = os.getenv(\"WORKSPACE_NAME\", default=\"YOUR_CURRENT_WORKSPACE\")\r\n","ws_region = os.getenv(\"WORKSPACE_REGION\", default=\"eastus2\")\r\n","\r\n","#Access to storage information\r\n","azure_storage_account_name = \"YOUR_STORAGE_ACCOUNT_NAME\"\r\n","azure_storage_account_key = \"YOUR_STORAGE_ACCOUNT_KEY\""],"outputs":[],"execution_count":4,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Connecting to a functional workspace \r\n","from azureml.core import Workspace\r\n","from azureml.core import Dataset, Datastore\r\n","from azureml.data.datapath import DataPath\r\n","\r\n","try:\r\n","    ws = Workspace(subscription_id = sub_id, resource_group = rsc_group, workspace_name = ws_name)\r\n","    # write the details of the workspace to a configuration file to the notebook library\r\n","    ws.write_config()\r\n","    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\r\n","except:\r\n","    print(\"Workspace not accessible. Change your parameters or create a new workspace below\")\r\n","\r\n","\r\n","# create file dataset from files in datastore\r\n","datastore = Datastore.get(ws, 'NAME_OF_THE_DATASTORE')\r\n","datastore_path = DataPath(datastore)\r\n","file_dataset = Dataset.File.from_files(path=datastore_path)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Performing interactive authentication. Please follow the instructions on the terminal.\n","To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code ANTQZTA3V to authenticate.\n","Interactive authentication successfully completed.\n","Workspace configuration succeeded. Skip the workspace creation steps below\n"]}],"execution_count":5,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Download the data into the AzureML compute instance. raw_data is an example folder name, you can create/name it whatever you feel like.\r\n","file_list = file_dataset.download(target_path=\"raw_data\")"],"outputs":[],"execution_count":6,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["## Feature Extraction\r\n","\r\n","For this project, we will be extracting drawings based on color using **OpenCV**. <br>\r\n","OpenCV loads images as *numpy* arrays, consisting of RGB values in a 2D matrix. <br>\r\n","Our goal is extracting the pixels that fall within our specified RGB range (BGR as OpenCV formats it this way) and use them to create a mask of the original image."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Simple directory navigation function, simplifies accessing a file's name from it's directory\r\n","def path_leaf(path):\r\n","    head, tail = nt.split(path)\r\n","    return tail"],"outputs":[],"execution_count":7,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["mask_list = []\r\n","def color_masking():\r\n","    \r\n","    \r\n","    #Iterating over the test files, would iterate over repo in the future\r\n","    for filename in glob.glob(\"./raw_data/*.jpg\"):\r\n","\r\n","        image = cv2.imread(filename)\r\n","        i = 1\r\n","\r\n","        #RGB boundaries for the opencv function of inRange.\r\n","        #These values are numpy arrays stored in reverse (BGR)\r\n","        #And represent the limits of what we consider a color. In this case, boundaries[0] = red, boundaries[1] = blue and boundaries[2] = green.\r\n","        boundaries = [\r\n","        ([17, 15, 100], [100, 106, 250]),\r\n","        ([86, 31, 4], [220, 88, 50]),\r\n","        ([57, 64, 36], [105, 125, 58])\r\n","        ]\r\n","\r\n","        for (lower, upper) in boundaries:\r\n","            lower = np.array(lower, dtype = \"uint8\")\r\n","            upper = np.array(upper, dtype = \"uint8\")\r\n","\r\n","            mask = cv2.inRange(image, lower, upper)\r\n","            mask_list.append(mask)\r\n","            output = cv2.bitwise_and(image, image, mask = mask)\r\n","\r\n","            newfile = path_leaf(filename)\r\n","            resultFile = newfile.split(\".\")\r\n","            \r\n","            #Depending on the iteration of the loop, the extracted colors vary, and as such the output image must be labelled accordingly.\r\n","            #This conditional statement serves that purpose. \r\n","            if i == 1:\r\n","                cv2.imwrite(\"./mask_output/\" + resultFile[0] + \"_red.jpg\", output)\r\n","            elif i == 2:\r\n","                cv2.imwrite(\"./mask_output/\" + resultFile[0] + \"_blue.jpg\", output)\r\n","            elif i == 3:\r\n","                cv2.imwrite(\"./mask_output/\" + resultFile[0] + \"_green.jpg\", output)\r\n","            i += 1\r\n","\r\n"],"outputs":[],"execution_count":17,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["## Creating the Color-Masked images"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["color_masking()"],"outputs":[],"execution_count":18,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["## Work in Progress:  Coordinate Referencing & Mask Clean Up\r\n","\r\n","The project is nearing completion, missing the coordinate transformation (from pixel coordinates to GPS coordinates) within the actual range of our map's coordinates.\r\n","The step after implies the processing of our masks, mainly to eliminate noise (drawings outside the map). <br>\r\n","\r\n","Next Steps:\r\n","- Accurately match the masks' features with referenced spatial coordinates (Azure Maps Image Layering)\r\n","- Clean up of noise (Marking the valid are and eliminating pixels outside of it)\r\n","- Possible: Implementation of Custom Vision model to differentiate between shapes' and their meaning (crosses from polygons)"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Test of coordinates being available\r\n","mask_sample = mask_list[0]\r\n","\r\n","print(\"Rows: \", len(mask_sample))\r\n","print(\"Columns: \", len(mask_sample[0]))\r\n","\r\n","result = np.where(mask_sample != 0)\r\n","listOfCoordinates= list(zip(result[0], result[1]))\r\n","print(len(listOfCoordinates))\r\n","for cord in listOfCoordinates:\r\n","    print(cord)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["LinkedIn: https://www.linkedin.com/in/gcordidoa/"],"metadata":{"nteract":{"transient":{"deleting":false}}}}],"metadata":{"kernelspec":{"name":"python3-azureml","language":"python","display_name":"Python 3.6 - AzureML"},"language_info":{"name":"python","version":"3.6.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernel_info":{"name":"python3-azureml"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":2}